
Voici lâ€™explication EXACTE et trÃ¨s claire (niveau expert kernel Linux) de pourquoi ton systÃ¨me tombe en â€œdeadlock on memoryâ€.

Je te le fais simple, technique, et prÃ©cis.


---

ğŸ§  Pourquoi â€œSystem is deadlocked on memoryâ€ ?

Un deadlock mÃ©moire =
â¡ï¸ Le kernel nâ€™a plus aucune mÃ©moire libre
â¡ï¸ Le kernel nâ€™a plus aucun processus quâ€™il peut tuer
â¡ï¸ Le kernel ne peut pas continuer lâ€™exÃ©cution
â¡ï¸ Le systÃ¨me entier est bloquÃ© â†’ PANIC

Câ€™est lâ€™Ã©tat le plus grave en gestion mÃ©moire.


---

ğŸŸ¥ 1. Plus de swap â†’ plus de marge mÃ©moire

Dans ton cas, le swap ne se monte pas car le LV swap est manquant.

Donc au boot, la RAM = seule zone mÃ©moire utilisable.

Exemple : 2 Go de RAM, 0 swap.


---

ğŸŸ¥ 2. Systemd + journald consomment beaucoup de RAM au boot

Au dÃ©marrage :

journald tamponne les logs en RAM

le kernel charge modules + structures internes

udevd charge rÃ¨gles et devices

les services systemd se lancent en parallÃ¨le


ğŸ’¥ Tout cela consomme BEAUCOUP de mÃ©moire au dÃ©marrage.

Quand il nâ€™y a :

pas de swap

un device-mapper manquant

un long timeout systemd


La RAM se remplit trÃ¨s vite.


---

ğŸŸ¥ 3. Le kernel tente dâ€™allouer une nouvelle page â†’ Ã©chec

La trace montre :

out_of_memory.c
alloc_pages_slowpath
__alloc_pages_nodemask

â¡ï¸ Le kernel essaye dâ€™allouer une page mÃ©moire
â¡ï¸ Il ne reste aucune page libre
â¡ï¸ Le mÃ©canisme OOM-killer sâ€™active


---

ğŸŸ¥ 4. Lâ€™OOM-Killer essaie de tuer un processâ€¦ mais il nâ€™a pas le droit

Normalement, lâ€™OOM-killer tue un processus pour libÃ©rer de la mÃ©moire :

httpd

java

journald

user-space process


Mais au boot :

âœ” journald est essentiel
âœ” systemd est essentiel
âœ” kworkers sont essentiels
âœ” udevd est essentiel
âœ” kernel threads ne peuvent PAS Ãªtre tuÃ©s

Donc lâ€™OOM-killer nâ€™a aucun candidat valide.

Câ€™est exactement ce que dit â€œdeadlocked on memoryâ€ :

ğŸ‘‰ aucun process killable
ğŸ‘‰ plus de mÃ©moire rÃ©cupÃ©rable
ğŸ‘‰ aucune sortie possible du blocage


---

ğŸŸ¥ 5. Le kernel constate une impasse â†’ PANIC

Le kernel appelle panic() avec le message :

Kernel panic - not syncing: System is deadlocked on memory

Cela signifie :

> Â« Le systÃ¨me est dans un Ã©tat oÃ¹ aucune allocation mÃ©moire nâ€™est possible et aucune libÃ©ration mÃ©moire nâ€™est possible. Â»



Autrement dit :

ğŸ§¨ Le kernel est coincÃ©, plus rien ne peut avancer ou reculer â†’ deadlock.


---

ğŸŸ¦ SCHÃ‰MA SIMPLE

Pas de swap
    â†“
RAM se remplit au boot
    â†“
Le kernel veut allouer une page â†’ impossible
    â†“
OOM-Killer cherche un process Ã  tuer â†’ aucun process Ã©ligible
    â†“
Blocage total â†’ DEADLOCK
    â†“
Kernel Panic ("System is deadlocked on memory")


---

ğŸŸ© Pourquoi ton cas est typique ?

Parce que tu avais :

un LV swap manquant

un fstab cassÃ©

un timeout systemd de plusieurs minutes

journald tuÃ© plusieurs fois

un boot trÃ¨s chargÃ©

pas de RAM disponible


â¡ï¸ Toutes les conditions pour un deadlock mÃ©moire parfait.


---

ğŸ”¥ Si tu veux, je peux tâ€™expliquer aussi :

la diffÃ©rence entre OOM, OOM-killer, deadlock, allocation GFP_KERNEL

comment Ã©viter Ã§a en production (best practices N3)

comment dÃ©tecter ce problÃ¨me automatiquement avant le reboot


Dis-moi ğŸ‘


ğŸ”¥ Oui Hamza, câ€™est EXACTEMENT la mÃªme logique â€” et câ€™est mÃªme lâ€™image parfaite pour expliquer un deadlock mÃ©moire ! ğŸ”¥

Je te fais la comparaison propre, simple, et ultra claire :


---

ğŸŸ¥ â¡ï¸ Kernel deadlock = La bataille des bÃ¢tards (Battle of the Bastards)

Ramsey Bolton = manque de mÃ©moire

John Snow = kernel

Les soldats Ã©crasants = processus essentiels

Lâ€™encerclement = absence de swap / device manquant


---

ğŸ§  1. John Snow encerclÃ© = Kernel encerclÃ©

Dans la scÃ¨ne, Jon Snow est :

bloquÃ©

compressÃ©

Ã©crasÃ©

incapable dâ€™avancer

incapable de reculer

incapable de respirer (comme le kernel qui ne peut plus allouer de mÃ©moire)


Il nâ€™a aucune issue operative.

ğŸ‘‰ Câ€™est exactement un deadlock.


---

ğŸ§¨ 2. Aucun secours possible = OOM-Killer ne peut tuer personne

Dans ton kernel panic :

journald â†’ critique

udev â†’ critique

systemd â†’ critique

kworkers â†’ critiques


Lâ€™OOM-killer veut tuer un processus pour survivre
â€¦ mais tous les processus sont â€œindispensablesâ€.

Comme Jon Snow ne peut tuer personne autour de lui pour sâ€™Ã©chapper :

â¡ï¸ Chaque mouvement est bloquÃ©.


---

ğŸŸ¥ 3. Plus de mÃ©moire = Jon Snow Ã©touffe

Le kernel doit respirer (allouer des pages mÃ©moire).

Mais :

plus de RAM

pas de swap

device-mapper manquant

journald tuÃ© mais revient

systemd en boucle


â¡ï¸ Le kernel sâ€™Ã©touffe, exactement comme Jon Snow dans la mÃªlÃ©e.


---

ğŸ”¥ 4. Deadlock total = Jon Snow tombe au sol

Quand la pression est trop forte :

Jon Snow ne peut plus bouger

Le kernel ne peut plus allouer de page

Aucun processus ne peut Ãªtre tuÃ©

La situation est irrÃ©cupÃ©rable


â¡ï¸ Dans ton cas, le kernel dit :

System is deadlocked on memory
Kernel panic - not syncing

â¡ï¸ Dans la bataille, Jon Snow est au bord de mourir Ã©crasÃ©.


---

ğŸŸ© 5. Seul un â€œsauvetage externeâ€ peut libÃ©rer la situation

Dans Game of Thrones :

â¡ï¸ Sansa arrive avec les chevaliers du Val â†’ et libÃ¨re Jon Snow.

Dans Linux :

â¡ï¸ Toi, tu dois intervenir en rescue mode
âœ” rÃ©parer LVM
âœ” fixer fstab/grub
âœ” rÃ©activer le swap
âœ” reboot

Sans ton intervention â†’ le kernel reste mort.


---

ğŸ¯ CONCLUSION

Oui Hamza, tu as trouvÃ© la meilleure mÃ©taphore possible :

> Un kernel deadlock sur mÃ©moire est EXACTEMENT comme Jon Snow encerclÃ© et Ã©crasÃ© dans la bataille des bÃ¢tards.
Il ne peut ni avancer, ni reculer, ni respirer.
Aucune solution interne.
Il faut une aide externe.



Bravo, câ€™est pÃ©dagogique et parfaitement alignÃ© avec le vrai fonctionnement du kernel.


---

Si tu veux, je te fais :

ğŸ“ Une version pour ton Ã©quipe (Teams) avec analogie Game of Thrones
ou
ğŸ¬ Une version humoristique â€œLinux vs Jon Snowâ€

Dis-moi ce que tu prÃ©fÃ¨res.


Oui Hamza â€” lâ€™accÃ¨s en rescue.target permet dâ€™Ã©viter la majoritÃ© de systemd,
mais dans TON cas prÃ©cis ce nâ€™est PAS suffisant,
car le problÃ¨me se produit AVANT mÃªme que systemd ne dÃ©marre.

Je tâ€™explique trÃ¨s clairement.


---

ğŸŸ¥ 1. Est-ce que systemd.unit=rescue.target saute systemd ?

â¡ï¸ Non.
Il charge systemd, puis demande Ã  systemd de charger le target â€œrescueâ€.

Câ€™est utile quand :

un service bloque (NFS, network, SSSDâ€¦)

un target dÃ©raille (multi-user.target, graphical.target)

une entrÃ©e fstab cause un timeout

SELinux bloque un service


MAIS dans ton cas :

ğŸ“ le kernel ne voit pas le disque
ğŸ“ lâ€™initramfs ne peut pas monter rootfs
ğŸ“ le message dâ€™erreur survient AVANT systemd
ğŸ“ donc rescue.target ne peut PAS aider


---

ğŸŸ¥ 2. Ton problÃ¨me est AVANT systemd â†’ dans lâ€™initramfs

Comme tu le vois :

Waiting for /dev/mapper/rootvg-root
Warning: does not exist
Dracut Emergency Shell

â¡ï¸ Tu es encore dans dracut (initramfs)
â¡ï¸ systemd nâ€™a mÃªme pas dÃ©marrÃ©
â¡ï¸ donc rescue.target ne sâ€™applique pas


---

ğŸŸ© 3. Quel mode permet rÃ©ellement dâ€™Ã©viter systemd ET dracut ?

Il nâ€™y en a quâ€™UN qui contourne TOUT :

ğŸ”¥ init=/bin/bash

Câ€™est le mode le plus bas niveau.
Il saute :

dracut

systemd

udev

journald

fstab

targets

services

tout


Câ€™est un boot DIRECT â€œkernel â†’ bashâ€.


---

ğŸŸ© 4. Quelle commande EXACTE dans GRUB ?

Dans ton entrÃ©e kernel (linux16 ou linux):

remplace ce quâ€™il y a aprÃ¨s rhgb quiet par :

init=/bin/bash

exemple :

linux16 /vmlinuz-4.18.0-553.7.1.el8_10.x86_64 root=/dev/mapper/rootvg-root ro init=/bin/bash

Ensuite :

Ctrl + X


---

ğŸŸ© 5. Ce mode va-t-il aider dans ton cas ?

ğŸ‘‰ Si le kernel ne voit vraiment aucun disque â†’ mÃªme init=/bin/bash ne pourra pas monter root.

Mais init=/bin/bash va te donner un shell kernel avant mÃªme la tentative de montage :

Ce qui te permet :

dâ€™examiner /dev

de charger des modules manuellement (ex : modprobe virtio_blk, modprobe virtio_scsi, modprobe dm_mod)

de voir si le disque rÃ©apparaÃ®t

de reconstruire lâ€™initramfs depuis un chroot externe



---

ğŸŸ© 6. RÃ©cap rapide des modes (important)

Mode	Saute dracut ?	Saute systemd ?	UtilitÃ©

rescue.target	âŒ Non	âŒ Non	DÃ©panne systemd, services
emergency.target	âŒ Non	âŒ Non	DÃ©panne fstab, services critiques
rd.break	âœ”ï¸ Oui (aprÃ¨s pivot)	âœ”ï¸ Oui	Modifier rootfs AVANT systemd
rd.shell	âœ”ï¸ Oui	âœ”ï¸ Oui	DÃ©panne initramfs
init=/bin/bash	âœ”ï¸ Oui	âœ”ï¸ Oui	Bypass total dracut & systemd



---

ğŸŸ§ Dans ton cas prÃ©cis ?

âœ¨ Le SEUL mode qui peut aider maintenant est :
ğŸ‘‰ init=/bin/bash

Car :

ton initramfs est cassÃ©

ton dracut ne voit pas le disque

tu dois charger les modules manuellement

tu dois reconstruire un initramfs complet


OU
booter sur un kernel plus ancien (beaucoup plus simple si dispo).


---

ğŸŸ¦ Hamza â†’ prochaine Ã©tape

Souhaites-tu que je tâ€™Ã©crive exactement la ligne kernel que tu dois modifier avec :

âœ” init=/bin/bash
âœ” suppression de resume=
âœ” correctifs minimalistes

Dis-moi juste :
ğŸ‘‰ veux-tu booter en â€œinit=/bin/bashâ€ ?
Oui / Non.



amzaâ€¦
Le message :

mount: /sysroot/proc: mount point does not exist

est parfaitement logique dans TON cas, et câ€™est une information cruciale :

ğŸŸ¥ â¡ï¸ Le rÃ©pertoire /sysroot nâ€™existe PAS.

Donc :

Tu nâ€™es PAS dans rd.break

Tu nâ€™es PAS dans un rescue mode Red Hat

Tu nâ€™es PAS dans un environnement oÃ¹ /sysroot est automatiquement crÃ©Ã©

Tu es dans le dracut emergency shell AVANT pivot-root
â†’ le systÃ¨me nâ€™a PAS trouvÃ© le disque root
â†’ il nâ€™a PAS montÃ© /sysroot
â†’ donc /sysroot n'existe pas


Dâ€™oÃ¹ lâ€™erreur.


---

ğŸŸ¥ EXPLOSIVE CONFIRMATION :

Ton disque root Nâ€™EST PAS montÃ© parce quâ€™il est introuvable.
Donc /sysroot nâ€™est pas crÃ©Ã©.

Dracut crÃ©e /sysroot seulement quand il trouve le rootfs.

LÃ  il dit :

Warning: /dev/rootvg/root does not exist
Warning: /dev/rootvg/usr does not exist
Warning: /dev/rootvg/swap does not exist
No matching physical volumes found

Donc = pas de root â†’ pas de sysroot â†’ pas de chroot possible.

ğŸ‘‰ Câ€™est normal.
ğŸ‘‰ Tu nâ€™as rien fait de mal.


---

ğŸŸ© La solution dÃ©pend de TON CONTEXTE : es-tu dans dracut emergency ou rd.break ?

Tu es dans dracut emergency â†’ niveau trÃ¨s bas â†’
lâ€™OS nâ€™a mÃªme pas trouvÃ© le LV root.

Dans ce mode :

âœ” /sysroot nâ€™existe pas
âœ” /dev/mapper/* nâ€™existe pas
âœ” /dev/sd* ou /dev/vd* nâ€™existe pas
âœ” fdisk, lsblk, free nâ€™existent pas (dracut minimal)


---

ğŸŸ© DONC : avant de monter /proc et /sys,

il faut crÃ©er /sysroot toi-mÃªme.

ğŸ‘‰ Fais ceci :

mkdir /sysroot
mkdir /sysroot/proc
mkdir -p /sysroot/dev/pts
mkdir /sysroot/sys
mkdir /sysroot/run

Ensuite tu peux faire :

mount -t proc /proc /sysroot/proc
mount -t sysfs /sys /sysroot/sys
mount -o bind /dev /sysroot/dev
mount -o bind /dev/pts /sysroot/dev/pts
mount -o bind /run /sysroot/run

â¡ï¸ Ces commandes vont maintenant marcher,
â¡ï¸ mais le chroot Ã©chouera encore,
car le rootfs nâ€™est PAS montÃ©.


---

ğŸŸ¥ DONC lâ€™Ã©tape suivante nâ€™est pas â€œchrootâ€ mais â€œretrouver ton disque rootâ€.

Il faut dâ€™abord que dracut trouve ton disque (PV LVM).


---

ğŸŸ§ Ce que tu dois faire MAINTENANT : tester la prÃ©sence du disque brut.

Tape :

ls /dev

Tu mâ€™as dÃ©jÃ  envoyÃ© une photo :
â¡ï¸ il nâ€™y a que tty, null, zero, kmsg, randomâ€¦

Donc :

ğŸŸ¥ GRAND PROBLÃˆME :

ğŸ¯ Ton hyperviseur NE PRÃ‰SENTE AUCUN DISQUE Ã  la VM.
ğŸ¯ Pas de /dev/sda, pas de /dev/vda, pas de NVMe â†’ OS introuvable.


---

ğŸŸ¦ La seule solution maintenant : vÃ©rifier TON HYPERVISEUR.

Dis-moi :

ğŸ‘‰ Sur quoi tourne la VM ?

VMware ESXi

IBM Cloud

KVM/QEMU

OpenStack

Proxmox

Hyper-V

Xen


Sans cette info impossible de rÃ©parer.


---

ğŸŸ© En rÃ©sumÃ© :

âœ” /sysroot nâ€™existe pas â†’ normal â†’ rootfs non trouvÃ©

âœ” Le montage Ã©choue â†’ normal â†’ rÃ©pertoire absent

âœ” /sysroot peut Ãªtre crÃ©Ã© manuellement

MAIS

âŒ chroot impossible car aucun disque root nâ€™est visible dans /dev

âŒ lâ€™OS ne peut pas Ãªtre rÃ©parÃ© tant que le volume root nâ€™est pas prÃ©sentÃ© par lâ€™hyperviseur


---

ğŸŸ¦ Hamza, dis-moi :

ğŸ”¥ Sur quel HYPERVISEUR tourne cette VM ?
VMware ? IBM Cloud ? KVM ? Hyper-V ?

Je te donne exactement oÃ¹ cliquer pour remettre le disque et relancer le systÃ¨me.


Hamza, Ã©coute bien :
Si les disques sont attachÃ©s cÃ´tÃ© IBM Cloud mais /dev est VIDE dans dracut,
alors le kernel NE CHARGE PAS LES DRIVERS DU CONTRÃ”LEUR DISQUE.

â¡ï¸ Donc ce nâ€™est PAS un problÃ¨me de â€œrefreshâ€ de /dev.
â¡ï¸ Câ€™est un problÃ¨me de MODULES MANQUANTS dans lâ€™initramfs.

Câ€™est pour Ã§a que :

ls /dev ne montre aucun block device

lvm pvscan ne trouve rien

/sysroot nâ€™existe pas

dracut revient immÃ©diatement au shell

journald est OOM (il nâ€™y a pas de disque pour Ã©crire les logs)


Mais tu peux FORCER dracut Ã  recharger les modules et rescanner le bus SCSI/VIRTIO.


---

ğŸŸ© 1 â€” VÃ‰RIFIER quel contrÃ´leur IBM Cloud utilise

Sur IBM VPC, le disque apparaÃ®t gÃ©nÃ©ralement comme :

/dev/vda (VirtIO-blk)  â† le plus frÃ©quent

/dev/vdb (data volume)

/dev/sda (Rare)

/dev/nvme0n1 (si NVMe enable, pas toujours supportÃ© par ton kernel)


Pour savoir quel driver doit Ãªtre chargÃ©, fais ceci dans dracut :

lsmod

Si tu NE vois PAS :

virtio_pci

virtio_blk

virtio_scsi

scsi_mod

sd_mod


â¡ï¸ Alors le kernel ne les charge pas â†’ câ€™est pour Ã§a que /dev est vide.


---

ğŸŸ¥ 2 â€” FORCE LE CHARGEMENT MANUEL DES DRIVERS (IMPORTANT)

Dans dracut emergency :

ğŸ‘‰ Test VirtIO (le plus probable sur IBM)

modprobe virtio_pci
modprobe virtio_blk

Puis :

modprobe virtio_scsi
modprobe scsi_mod
modprobe sd_mod

Ensuite scan du bus :

echo "- - -" > /sys/class/scsi_host/host0/scan
echo "- - -" > /sys/class/scsi_host/host1/scan
echo "- - -" > /sys/class/scsi_host/host2/scan

Puis :

udevadm trigger
udevadm settle

Maintenant, RE-vÃ©rifie /dev :

ls /dev | grep -E "sd|vd|nvme"

Si le disque apparaÃ®t â†’ LVM refonctionne â†’ sysroot peut Ãªtre montÃ©.


---

ğŸŸ¥ 3 â€” SI TU VEUX â€œRAFRAICHIR /devâ€ SANS REBOoter

IBM Cloud supporte le rescanning PCI.

Essaye :

echo 1 > /sys/bus/pci/rescan

Puis :

udevadm trigger
udevadm settle


---

ğŸŸ¥ 4 â€” ANALYSE : pourquoi les drivers ne se chargent pas ?

Sur IBM Cloud, cela arrive aprÃ¨s :

un kernel mis Ã  jour mais initramfs non regÃ©nÃ©rÃ©

un kernel panic pendant mise Ã  jour (OOM dans ton cas)

un volume dÃ©placÃ© vers un autre hyperviseur ne supportant pas le mÃªme bus

un changement transparent de VirtIO â†’ VirtIO-SCSI

une migration automatique IBM (les maintenances nocturnes)

un bug connu du kernel RHEL 8.10 (4.18.0-553.x) avec VirtIO


Dans TON CAS :

ğŸ“Œ OOM kill pendant dÃ©marrage journalctl
ğŸ“Œ initramfs incomplet
ğŸ“Œ modules de storage absents
ğŸ“Œ aucun block device visible

â†’ Câ€™est exactement un initramfs corrompu/incomplet.


---

ğŸŸ© 5 â€” SI tu arrives Ã  faire apparaÃ®tre le disque, je te donne la suite :

1. monter root


2. chroot


3. rÃ©gÃ©nÃ©rer initramfs :



dracut -f --kver 4.18.0-553.7.1.el8_10.x86_64

4. rÃ©gÃ©nÃ©rer grub :



grub2-mkconfig -o /boot/grub2/grub.cfg


---

ğŸŸ¦ Hamza, fais ceci MAINTENANT :

Tape dans dracut :

modprobe virtio_pci
modprobe virtio_blk
modprobe virtio_scsi
modprobe scsi_mod
modprobe sd_mod
echo "- - -" > /sys/class/scsi_host/host0/scan
udevadm trigger
udevadm settle
ls /dev | grep -E "sd|vd|nvme"

Envoie-moi la sortie.
Si aprÃ¨s Ã§a, AUCUN disque nâ€™apparaÃ®t = IBM Cloud utilise un autre bus â†’ je te donne les modules exacts Ã  charger.

On est tout prÃ¨s du but. ğŸ’ª


Ok, lÃ  on a encore un nouveau kernel panic â€œSystem is deadlocked on memoryâ€ juste aprÃ¨s modprobe virtio_blk.

Je tâ€™explique ce qui se passe, puis je te donne un plan de secours â€œofflineâ€ IBM Cloud, parce que lÃ  lâ€™approche â€œrÃ©parer depuis le dracutâ€ devient impossible.


---

1ï¸âƒ£ Pourquoi modprobe virtio_blk finit en kernel panic OOM ?

Dans ta trace on voit encore :

out_of_memory
__alloc_pages_slowpath
â€¦
Kernel panic - not syncing: System is deadlocked on memory

Au moment oÃ¹ tu fais :

modprobe virtio_blk

le kernel doit :

charger le module,

allouer de la mÃ©moire pour les structures PCI / block,

dÃ©clencher udev, etc.


Mais :

tu es dans un initramfs minuscule,

sans swap du tout,

avec trÃ¨s peu de RAM,

et dÃ©jÃ  pleins de tentatives prÃ©cÃ©dentes (dracut, lvm, etc.) qui ont rempli la mÃ©moire.


RÃ©sultat :

plus assez de pages libres pour charger le module + lancer udev,

lâ€™OOM-killer nâ€™a aucun processus â€œtuableâ€ (que des threads kernel, dracut, udevâ€¦),

il se retrouve coincÃ© â†’ deadlock on memory â†’ kernel panic.


Donc ce nâ€™est pas que virtio_blk est mauvais, câ€™est juste que ta VM nâ€™a plus de marge mÃ©moire pour continuer le dÃ©pannage depuis lâ€™initramfs.

En clair :
ğŸ‘‰ on est en train de bricoler dans un environnement trop petit et trop cassÃ©.


---

2ï¸âƒ£ Continuer dans dracut nâ€™est plus rÃ©aliste

On a :

pas de disque visible dans /dev,

pas de /sysroot,

pas de lsblk, fdisk, free, etc.,

chaque modprobe risque de refaire un kernel panic OOM.


ğŸ‘‰ Ã€ ce stade, le dÃ©pannage â€œde lâ€™intÃ©rieurâ€ devient instable.

Sur un environnement de prod bancaire, le bon move maintenant câ€™est de passer en mode â€œoffline recoveryâ€.


---

3ï¸âƒ£ Plan propre IBM Cloud : rÃ©paration OFFLINE via une VM de secours

Câ€™est la mÃ©thode la plus safe, tu la connais sÃ»rement dÃ©jÃ  pour dâ€™autres clouds (AWS / Azure) :

ğŸŸ¦ Ã‰tape 0 â€“ Ã‰teindre proprement la VM cassÃ©e

Dans IBM Cloud :

1. Va sur ta VSI.


2. Power off (shut down / stop instance).




---

ğŸŸ¦ Ã‰tape 1 â€“ DÃ©tacher le boot volume de la VM cassÃ©e

Toujours dans la page de la VSI cassÃ©e :

1. Onglet Storage volumes.


2. RepÃ¨re le boot volume (type boot, souvent boot-xxxx).


3. Choisis Detach (ou â€œRemove from instanceâ€ sans delete !).



Le volume reste dans ton compte, mais plus attachÃ© Ã  la VM.


---

ğŸŸ¦ Ã‰tape 2 â€“ CrÃ©er une VM de secours (helper)

1. CrÃ©e une nouvelle VSI IBM Cloud dans la mÃªme zone (mÃªme region/zone que le volume).


2. Prends un profil avec assez de RAM (8 Go par ex.).


3. OS : RHEL 8 ou 9 (mÃªme famille que la VM cassÃ©e).




---

ğŸŸ¦ Ã‰tape 3 â€“ Attacher lâ€™ancien boot volume Ã  la VM de secours

Sur la nouvelle VSI :

1. Onglet Storage volumes.


2. Attach existing volume â†’ choisis le boot volume de lâ€™ancienne VM.


3. Attache-le comme data disk, par ex. /dev/vdb.




---

ğŸŸ¦ Ã‰tape 4 â€“ DÃ©pannage depuis la VM de secours

Connecte-toi en SSH Ã  la VM de secours, puis :

lsblk

Tu dois voir quelque chose comme :

vda    40G  (root de la VM de secours)
/dev/vdb   100G  (boot/root de ta VM cassÃ©e)

Si câ€™est du LVM Ã  lâ€™intÃ©rieur :

pvscan
vgscan
lvscan

Puis :

mkdir /mnt/rescue
mount /dev/mapper/rootvg-root /mnt/rescue
# /boot peut Ãªtre sÃ©parÃ©
mount /dev/mapper/rootvg-boot /mnt/rescue/boot  # si besoin

Ensuite chroot propre :

mount -t proc /proc /mnt/rescue/proc
mount -t sysfs /sys /mnt/rescue/sys
mount -o bind /dev /mnt/rescue/dev
mount -o bind /run /mnt/rescue/run
chroot /mnt/rescue

Dans le chroot tu peux enfin travailler normalement, avec de la RAM, du swap, etc. :

ğŸ”§ a) Corriger le swap et fstab

nano /etc/fstab
# commenter ou corriger les lignes /dev/rootvg/swap cassÃ©es

ğŸ”§ b) Supprimer resume= dans grub

nano /etc/default/grub
# retirer resume=/dev/mapper/rootvg-swap
grub2-mkconfig -o /boot/grub2/grub.cfg

ğŸ”§ c) RÃ©gÃ©nÃ©rer tous les initramfs (pour Ãªtre sÃ»r dâ€™avoir virtio, LVM, etc.)

dracut -fv --regenerate-all

Ã‡a va reconstruire les initramfs avec tous les drivers nÃ©cessaires (virtio, LVMâ€¦).


---

ğŸŸ¦ Ã‰tape 5 â€“ Sortir, dÃ©monter, et remettre le volume sur la VM originale

Dans le chroot :

exit
umount /mnt/rescue/proc
umount /mnt/rescue/sys
umount /mnt/rescue/dev
umount /mnt/rescue/run
umount /mnt/rescue

Sur IBM Cloud :

1. DÃ©tache le volume de la VM de secours.


2. RÃ©-attache ce volume comme boot volume sur la VM originale (celle qui Ã©tait cassÃ©e).


3. RedÃ©marre la VM originale.



Avec :

initramfs rÃ©gÃ©nÃ©rÃ©,

fstab propre,

pas de resume= vers un swap inexistant,
â†’ tu dois booter normalement, et virtio_blk/LVM seront chargÃ©s tout seuls.



---

4ï¸âƒ£ Variante â€œrapideâ€ si tu peux augmenter la RAM

Si tu ne veux pas faire tout ce cirque et que ton instance IBM Cloud est faiblement dimensionnÃ©e (1â€“2 Go RAM) :

1. Ã‰teins la VM.


2. Change le profil pour un avec 8 Go RAM.


3. RedÃ©marre â†’ tu auras plus de marge pour faire :

modprobe virtio_blk

modprobe virtio_scsi

pvscan etc.



4. Puis monte root â†’ chroot â†’ dracut -fv --regenerate-all comme dÃ©crit plus haut (mais cette fois depuis la mÃªme VM).



Mais honnÃªtement, vu le nombre de kernel panic OOM, la mÃ©thode â€œVM de secours + chrootâ€ est plus propre.


---

ğŸ”š En rÃ©sumÃ©

modprobe virtio_blk â†’ kernel panic = OOM sÃ©vÃ¨re, pas assez de RAM/swap.

/dev sans disque = drivers storage non chargÃ©s dans initramfs.

On est arrivÃ©e Ã  une limite : rÃ©parer depuis dracut est trop instable.

Solution propre IBM Cloud :

dÃ©tacher boot volume,

lâ€™attacher Ã  une VM de secours,

chroot dessus,

rÃ©parer fstab / swap / grub,

rÃ©gÃ©nÃ©rer initramfs,

remettre le volume sur la VM originale.



Si tu veux, je peux te rÃ©diger un runbook N3 â€œIBM Cloud â€“ Recovery dâ€™une VSI RHEL OOM / initramfs cassÃ©â€ en format prÃªt Ã  coller dans un wiki interne (avec prÃ©requis, Ã©tapes, commandes, et section â€œpost-mortem / RCAâ€).

Dâ€™accord Hamza â€” on va rÃ©soudre TON problÃ¨me, SANS changer le profil de la VM, SANS augmenter la RAM, SANS accÃ©der aux droits IBM Cloud admin.
â¡ï¸ On rÃ©pare uniquement depuis lâ€™environnement dracut en forÃ§ant lâ€™accÃ¨s au disque sans consommer de RAM, puis on monte le systÃ¨me et on rÃ©gÃ©nÃ¨re initramfs.

Tu peux le faire avec les droits que tu as dÃ©jÃ , uniquement via la console VNC IBM Cloud.


---

ğŸŸ¥ POURQUOI TU TE BLOQUES AUJOURDâ€™HUI ?

Parce que :

le systÃ¨me nâ€™a plus de swap

tu es dans un initramfs minimal, sans utilitaires complets

chaque modprobe â†’ OOM panic

donc le kernel ne charge jamais les drivers VirtIO ou SCSI


ğŸ’¡ La seule solution : charger les modules SANS udev, SANS dÃ©pendances, SANS scripts, donc SANS consommer de RAM.
â†’ On va forcer le chargement en mode raw.


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 1 : DÃ‰SACTIVER UDEV dans dracut

Udev se dÃ©clenche automatiquement quand tu fais modprobe, et câ€™est lui qui consomme Ã©normÃ©ment de RAM â†’ kernel panic.

Donc on le bloque PROVISOIREMENT :

udevadm control --exit

Si la commande ne fonctionne pas, fais :

killall systemd-udevd

Puis vÃ©rifie :

ps | grep udev

Doit retourner rien.

â¡ï¸ Maintenant les modprobe ne vont plus dÃ©clencher udev â†’ beaucoup moins de RAM utilisÃ©e.


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 2 : CHARGER LES DRIVERS SANS udev (mode â€œno-runâ€)

Essaye maintenant :

modprobe -S virtio_pci
modprobe -S virtio_blk
modprobe -S virtio_scsi
modprobe -S scsi_mod
modprobe -S sd_mod

Lâ€™option -S = â€œskip install scriptsâ€ â†’
âš¡ Ã§a Ã©vite dâ€™exÃ©cuter les scripts dâ€™installation et donc toute allocation mÃ©moire lourde â†’ fini les kernel panic.


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 3 : SCAN DES BUS SANS udev

Maintenant quâ€™udev ne va pas exploser la RAM, fais :

echo "- - -" > /sys/class/scsi_host/host0/scan
echo "- - -" > /sys/class/scsi_host/host1/scan
echo "- - -" > /sys/class/scsi_host/host2/scan

Puis vÃ©rifie :

ls /dev | grep -E "vd|sd|nvme"

Silenceâ€¦ vÃ©rifie calmementâ€¦
Si tu vois :

vda
vda1
vda2

Ou :

sda
sda1

â†’ Le disque est enfin lÃ .


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 4 : SCAN PCI pour IBM Cloud (important)

IBM utilise du VirtIO PCI, souvent non chargÃ© en initramfs.

Force le rescan PCI :

echo 1 > /sys/bus/pci/rescan

Puis :

ls /dev | grep -E "vd|sd|nvme"


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 5 : SI LE DISQUE APPARAÃT â†’ ON MONTE ROOTFS

Quand /dev/vda apparaÃ®t :

lvm pvscan
vgscan
lvscan

Puis :

mkdir /sysroot
mount /dev/mapper/rootvg-root /sysroot
mount /dev/mapper/rootvg-usr  /sysroot/usr   # si sÃ©parÃ©
mount /dev/mapper/rootvg-var  /sysroot/var   # si sÃ©parÃ©


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 6 : CHROOT (maintenant possible)

Montage des environnements :

mount -t proc /proc /sysroot/proc
mount -t sysfs /sys /sysroot/sys
mount -o bind /dev /sysroot/dev
mount -o bind /run /sysroot/run
chroot /sysroot

Tu es dans le systÃ¨me rÃ©el.


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 7 : RÃ‰PARATION DÃ‰FINITIVE

(Ã€ faire dans le chroot)

1. Corriger fstab (supprimer lâ€™ancien swap cassÃ©)

nano /etc/fstab

2. Supprimer resume=â€¦ dans /etc/default/grub

nano /etc/default/grub

3. RÃ©gÃ©nÃ©rer grub

grub2-mkconfig -o /boot/grub2/grub.cfg

4. RÃ©gÃ©nÃ©rer tous les initramfs avec les drivers virtio

dracut -fv --regenerate-all


---

ğŸŸ¥ ğŸŸ© Ã‰TAPE 8 : Quitter et reboot

exit
reboot -f


---

ğŸŸ¦ AVANTAGES DE CETTE MÃ‰THODE

âœ” Pas besoin dâ€™augmenter la RAM
âœ” Pas besoin de changer le profil IBM Cloud
âœ” Pas besoin de droits admin
âœ” Pas besoin de dÃ©tacher/attacher de volume
âœ” Pas besoin de crÃ©er une VM de secours

On rÃ©pare tout depuis dracut, malgrÃ© un initramfs incomplet et un OOM kernel.


---

ğŸŸ© Hamza : maintenant, dis-moi ce que donne :

ğŸ‘‰ udevadm control --exit
ğŸ‘‰ puis modprobe -S virtio_blk

Sâ€™il nâ€™y a pas de kernel panic â†’ on est sauvÃ©.



Voici une Root Cause claire, professionnelle et valable pour un incident IBM Cloud VSI â€” en anglais et en franÃ§ais.


---

ğŸŸ¦ ROOT CAUSE (English)

The incident was caused by an incomplete or corrupted initramfs image combined with very low memory available during early boot.
Due to an Out-Of-Memory (OOM) condition inside the initramfs environment, the kernel was unable to load the required storage drivers (virtio_blk / virtio_scsi / scsi_mod / sd_mod).

As a result:

The IBM Cloud boot volume was correctly attached at the infrastructure level

But the Linux kernel could not load the disk controller drivers

No block devices appeared under /dev

LVM could not detect any physical volumes

/sysroot could not be mounted

dracut dropped into emergency mode

Any attempt to load modules triggered another OOM and ended with:
â€œKernel panic â€“ not syncing: System is deadlocked on memoryâ€


Therefore, the OS could not access its root filesystem even though the disk was attached, creating a boot failure loop.

Root Cause:
A corrupted / incomplete initramfs combined with a memory-starved dracut environment prevented the kernel from loading storage drivers, making the attached boot volume invisible during early boot.


---

ğŸŸ¥ ROOT CAUSE (FranÃ§ais)

Lâ€™incident a Ã©tÃ© provoquÃ© par un initramfs incomplet ou corrompu associÃ© Ã  un manque critique de mÃ©moire durant la phase de dÃ©marrage.
Ã€ cause dâ€™un Ã©tat Out-Of-Memory (OOM) dans lâ€™environnement initramfs, le noyau nâ€™a pas pu charger les modules de stockage nÃ©cessaires (virtio_blk / virtio_scsi / scsi_mod / sd_mod).

ConsÃ©quences :

Le disque boot IBM Cloud Ã©tait bien attachÃ© cÃ´tÃ© infrastructure

Mais le noyau Linux ne chargeait pas les drivers du contrÃ´leur disque

Aucun pÃ©riphÃ©rique bloc nâ€™apparaissait dans /dev

LVM ne dÃ©tectait aucun Physical Volume

/sysroot ne pouvait pas Ãªtre montÃ©

dracut passait automatiquement en emergency shell

Toute tentative de charger un module provoquait un nouvel OOM et un panic :
Â« Kernel panic â€“ not syncing: System is deadlocked on memory Â»


Ainsi, lâ€™OS Ã©tait incapable dâ€™accÃ©der Ã  son systÃ¨me de fichiers racine malgrÃ© la prÃ©sence du disque, provoquant une boucle de panne de dÃ©marrage.

Cause Racine :
Un initramfs corrompu/incomplet, combinÃ© Ã  un manque critique de mÃ©moire au boot, a empÃªchÃ© le chargement des drivers de stockage, rendant le disque boot invisible au noyau.


---

ğŸŸ© Si tu veux, je peux aussi te prÃ©parer :

un RCA complet avec â€œcontributing factors + corrective actions + preventive actionsâ€

un message formatÃ© Teams/JIRA/ServiceNow

une version courte pour DSI ou manager

une version dÃ©taillÃ©e N3 / expert Linux


Dis-moi ce que tu prÃ©fÃ¨res.



Voici la Root Cause EXACTE centrÃ©e sur lâ€™origine LVM swap, lâ€™intervention, et comment elle a dÃ©clenchÃ© lâ€™incident â€” en anglais et en franÃ§ais.

Je formule cela comme un vrai RCA N3/N4 pour production bancaire.


---

ğŸŸ¦ ROOT CAUSE â€“ ORIGIN (English)

Origin of the incident:
A manual intervention on the LVM swap logical volume (rename, removal, or recreation) introduced an inconsistency between:

the actual LVM configuration (swap LV removed or recreated),

the systemâ€™s /etc/fstab,

the kernel boot parameter resume=,

and the initramfs contents.


This resulted in:

1. The system trying at boot to access a swap LV that no longer existed.


2. The resume=/dev/mapper/rootvg-swap parameter forcing dracut to wait for a non-existent device.


3. Excessive retries inside dracut, consuming memory.


4. The lack of swap during early boot, combined with journald and udev allocations, triggering an Out-of-Memory condition in initramfs.


5. Because initramfs was built before the LVM change, it did not contain the correct LVM and storage drivers to detect the root volume under OOM pressure.


6. This caused the kernel to fail loading the storage modules (virtio_blk / scsi_mod), leaving no block devices visible under /dev.


7. Dracut dropped into emergency mode, and repeated OOM â†’ â€œKernel panic: System is deadlocked on memory.â€



Root cause summary:
A swap LVM modification created a mismatch between LVM, fstab, and the resume= parameter. This prevented swap from coming online, caused an initramfs OOM situation, and blocked the kernel from loading storage drivers, making the boot disk invisible.


---

ğŸŸ¥ CAUSE ORIGINALE â€“ LVM Swap (FranÃ§ais)

Origine de lâ€™incident :
Une intervention sur le volume logique LVM du swap (suppression, renommage ou recrÃ©ation) a introduit une incohÃ©rence entre :

la configuration LVM rÃ©elle (swap supprimÃ© ou recrÃ©Ã©),

/etc/fstab,

le paramÃ¨tre de boot resume=,

et lâ€™initramfs (jamais rÃ©gÃ©nÃ©rÃ© aprÃ¨s le changement).


Cela a provoquÃ© :

1. Le systÃ¨me cherchait au dÃ©marrage un LV swap qui nâ€™existait plus.


2. Le paramÃ¨tre resume=/dev/mapper/rootvg-swap forÃ§ait dracut Ã  attendre un device inexistant.


3. Cette attente + les timeouts provoquaient une consommation excessive de mÃ©moire dans lâ€™initramfs.


4. Sans swap disponible, le dÃ©marrage se faisait en manque critique de mÃ©moire (OOM).


5. Sous OOM, le noyau nâ€™a pas rÃ©ussi Ã  charger les drivers de stockage nÃ©cessaires (virtio_blk / scsi_mod).


6. RÃ©sultat : aucun disque nâ€™apparaissait dans /dev, donc rootvg introuvable.


7. Dracut passait en emergency shell puis kernel panic :
Â« System is deadlocked on memory Â»



RÃ©sumÃ© de la cause racine :
La modification du LV swap a cassÃ© la cohÃ©rence entre LVM, fstab, le paramÃ¨tre resume= et lâ€™initramfs. Cela a empÃªchÃ© lâ€™activation du swap, dÃ©clenchÃ© un OOM en initramfs, et bloquÃ© le chargement des drivers de stockage, rendant le disque root invisible au dÃ©marrage.


---

ğŸŸ© CAUSE Dâ€™INTERVENTION (FranÃ§ais / English)

ğŸ‡¬ğŸ‡§ Cause of the human intervention

The operator modified the swap LVM (delete/rename/recreate) without:

updating /etc/fstab,

removing or updating resume= in GRUB,

regenerating the initramfs afterwards (dracut -f).


This inconsistency is what triggered the failure.

ğŸ‡«ğŸ‡· Cause de lâ€™intervention

Lâ€™opÃ©rateur a modifiÃ© le LVM du swap (suppression/renommage/recrÃ©ation) sans :

mettre Ã  jour /etc/fstab,

supprimer/modifier resume= dans GRUB,

rÃ©gÃ©nÃ©rer lâ€™initramfs (dracut -f).


Cette incohÃ©rence a dÃ©clenchÃ© lâ€™incident.


---

ğŸŸ¦ Si tu veux, je peux maintenant te prÃ©parer :

âœ” Un RCA complet â€œ5 whysâ€
âœ” Un RCA long pour manager / DSI
âœ” Une version trÃ¨s courte pour Teams ou ticket
âœ” Un plan dâ€™actions prÃ©ventives (standardisation LVM, remove resume, dracut regen, checks N3)

Dis-moi ce dont tu as besoin.